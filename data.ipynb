{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "column_names = [\n",
    "    'ID', 'PARAM', 'TYPE', 'YEAR', 'DD',\n",
    "    'Jan', 'Jan_SYM', 'Feb', 'Feb_SYM', 'Mar', 'Mar_SYM',\n",
    "    'Apr', 'Apr_SYM', 'May', 'May_SYM', 'Jun', 'Jun_SYM',\n",
    "    'Jul', 'Jul_SYM', 'Aug', 'Aug_SYM', 'Sep', 'Sep_SYM',\n",
    "    'Oct', 'Oct_SYM', 'Nov', 'Nov_SYM', 'Dec', 'Dec_SYM'\n",
    "]\n",
    "\n",
    "MONTHS = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "data = pd.read_csv('data/credit_hydrometric_data_all_stations.csv', skiprows=2, names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_02HB018 = data[data['ID'] == '02HB018'].reset_index(drop=True)\n",
    "\n",
    "_02HB025 = data[data['ID'] == '02HB025'].reset_index(drop=True)\n",
    "\n",
    "_02HB031 = data[data['ID'] == '02HB031'].reset_index(drop=True)\n",
    "\n",
    "_02HB001 = data[data['ID'] == '02HB001'].reset_index(drop=True)\n",
    "\n",
    "_02HB013 = data[data['ID'] == '02HB013'].reset_index(drop=True)\n",
    "\n",
    "_02HB029 = data[data['ID'] == '02HB029'].reset_index(drop=True)  ### TARGET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import calendar\n",
    "\n",
    "def is_valid_date(year, month, day):\n",
    "    try:\n",
    "        m = calendar.monthrange(year, month)\n",
    "        if day > m[1]:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "\n",
    "\n",
    "def split_by_param_1_2(df: pd.DataFrame):\n",
    "    data1 = df[df['PARAM'] == 1].reset_index(drop=True)\n",
    "    data2 = df[df['PARAM'] == 2].reset_index(drop=True)\n",
    "    return data1, data2\n",
    "\n",
    "def wateroffice_to_timeseries(df : pd.DataFrame):\n",
    "    data_dict = {'timestamp': [], 'value': []}\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        year = row['YEAR']\n",
    "        day = row['DD']\n",
    "        for month in MONTHS :\n",
    "            if is_valid_date(year, MONTHS.index(month) + 1, day):\n",
    "\n",
    "                timestamp = pd.Timestamp(f'{year}-{month}-{day}')\n",
    "                val = df.at[i, f'{month}']\n",
    "                data_dict['timestamp'].append(timestamp)\n",
    "                data_dict['value'].append(val)\n",
    "\n",
    "    new_data = pd.DataFrame(data_dict)\n",
    "    return new_data\n",
    "\n",
    "\n",
    "def merge_param_1_2(df_left : pd.DataFrame, df_right : pd.DataFrame):\n",
    "    return pd.merge(df_left, df_right, on='timestamp', how='inner')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_02HB018_1, _02HB018_2 = split_by_param_1_2(_02HB018)\n",
    "_02HB025_1, _02HB025_2 = split_by_param_1_2(_02HB025)\n",
    "_02HB031_1, _02HB031_2 = split_by_param_1_2(_02HB031)\n",
    "_02HB001_1, _02HB001_2 = split_by_param_1_2(_02HB001)\n",
    "_02HB013_1, _02HB013_2 = split_by_param_1_2(_02HB013)\n",
    "_02HB029_1, _02HB029_2 = split_by_param_1_2(_02HB029)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_02HB018_1_ts = wateroffice_to_timeseries(_02HB018_1).rename(columns={'value': 'discharge_02HB018'})\n",
    "_02HB018_2_ts = wateroffice_to_timeseries(_02HB018_2).rename(columns={'value': 'water_level_02HB018'})\n",
    "_02HB025_1_ts = wateroffice_to_timeseries(_02HB025_1).rename(columns={'value': 'discharge_02HB025'})\n",
    "_02HB025_2_ts = wateroffice_to_timeseries(_02HB025_2).rename(columns={'value': 'water_level_02HB025'})\n",
    "_02HB031_1_ts = wateroffice_to_timeseries(_02HB031_1).rename(columns={'value': 'discharge_02HB031'})\n",
    "_02HB031_2_ts = wateroffice_to_timeseries(_02HB031_2).rename(columns={'value': 'water_level_02HB031'})\n",
    "_02HB001_1_ts = wateroffice_to_timeseries(_02HB001_1).rename(columns={'value': 'discharge_02HB001'})\n",
    "_02HB001_2_ts = wateroffice_to_timeseries(_02HB001_2).rename(columns={'value': 'water_level_02HB001'})\n",
    "_02HB013_1_ts = wateroffice_to_timeseries(_02HB013_1).rename(columns={'value': 'discharge_02HB013'})\n",
    "_02HB013_2_ts = wateroffice_to_timeseries(_02HB013_2).rename(columns={'value': 'water_level_02HB013'})\n",
    "_02HB029_1_ts = wateroffice_to_timeseries(_02HB029_1).rename(columns={'value': 'discharge_02HB029'})\n",
    "_02HB029_2_ts = wateroffice_to_timeseries(_02HB029_2).rename(columns={'value': 'water_level_02HB029'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_param_1_2(df_left : pd.DataFrame, df_right : pd.DataFrame):\n",
    "    return pd.merge(df_left, df_right, on='timestamp', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_02HB018 = merge_param_1_2(_02HB018_1_ts, _02HB018_2_ts)\n",
    "merged_02HB025 = merge_param_1_2(_02HB025_1_ts, _02HB025_2_ts)\n",
    "merged_02HB031 = merge_param_1_2(_02HB031_1_ts, _02HB031_2_ts)\n",
    "merged_02HB001 = merge_param_1_2(_02HB001_1_ts, _02HB001_2_ts)\n",
    "merged_02HB013 = merge_param_1_2(_02HB013_1_ts, _02HB013_2_ts)\n",
    "merged_02HB029 = merge_param_1_2(_02HB029_1_ts, _02HB029_2_ts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge into single dataframe with dischrage and water_level for all stations and add suffix to columns, dont have redundant columns\n",
    "df_merged = reduce(lambda  left,right: pd.merge(left,right,on=['timestamp'],\n",
    "                                            how='outer'), [merged_02HB018, merged_02HB025, merged_02HB031, merged_02HB001, merged_02HB013, merged_02HB029])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_merged['timestamp'].diff().dt.days.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.set_index('timestamp', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.ffill(inplace=True)\n",
    "df_merged.bfill(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "df_scaled = scaler.fit_transform(df_merged)\n",
    "\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset) - look_back):\n",
    "        a = dataset[i:(i + look_back), :]\n",
    "        X.append(a)\n",
    "        Y.append(dataset[i + look_back, -1])  # Assuming the last column is the target\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "look_back = 1\n",
    "X, Y = create_dataset(df_scaled, look_back)\n",
    "\n",
    "X = X.reshape((X.shape[0], X.shape[1], df_scaled.shape[1]))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(look_back, df_scaled.shape[1])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X, Y, epochs=100, batch_size=1, verbose=2)\n",
    "\n",
    "predictions = model.predict(X)\n",
    "predictions = scaler.inverse_transform(np.hstack((np.zeros((predictions.shape[0], df_scaled.shape[1]-1)), predictions)))\n",
    "\n",
    "Y = scaler.inverse_transform(np.hstack((np.zeros((Y.shape[0], df_scaled.shape[1]-1)), Y.reshape(-1, 1))))[:, -1]\n",
    "predictions = predictions[:, -1]\n",
    "\n",
    "rmse = math.sqrt(mean_squared_error(Y, predictions))\n",
    "print(f'Train Score: {rmse:.2f} RMSE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flood",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
