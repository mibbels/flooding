{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-15 09:13:44.214352: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-15 09:13:44.214389: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-15 09:13:44.214414: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-15 09:13:44.222363: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-15 09:13:45.620751: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-15 09:13:45.626495: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-15 09:13:45.626773: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, GRU, BatchNormalization, Bidirectional, LayerNormalization, ConvLSTM1D, Flatten, Dropout\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import calendar\n",
    "\n",
    "USE_GPU = 1\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' if USE_GPU else ''\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Constants\n",
    "MONTHS = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "def split_by_param(df : pd.DataFrame, param : str) -> dict:\n",
    "    \"\"\"\n",
    "    Splits the DataFrame based on the PARAM column into separate DataFrames for each unique PARAM value.\n",
    "    \"\"\"\n",
    "    return {p: df[df[param] == p].reset_index(drop=True) for p in df[param].unique()}\n",
    "\n",
    "def is_valid_date(year, month, day):\n",
    "    \"\"\"\n",
    "     check if day > last day of specific month + year\n",
    "    \"\"\"\n",
    "    try:\n",
    "        m = calendar.monthrange(year, month)\n",
    "        if day > m[1]:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "    \n",
    "def convert_to_timeseries(df, month_identifiers : list = MONTHS):\n",
    "    \"\"\"\n",
    "    Converts a DataFrame from wide to timeseries format.\n",
    "    1999 Day1 JAN FEB MAR ...\n",
    "    1999 Day2 JAN FEB MAR ...\n",
    "    \"\"\"\n",
    "    data_dict = {'timestamp': [], 'value': []}\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        year = row['YEAR']\n",
    "        day = row['DD']\n",
    "        for month in month_identifiers :\n",
    "            valid_date = is_valid_date(year, month_identifiers.index(month) + 1, day)\n",
    "\n",
    "            if valid_date:\n",
    "                timestamp = pd.Timestamp(f'{year}-{month}-{day}')\n",
    "                val = df.at[i, f'{month}']\n",
    "                data_dict['timestamp'].append(timestamp)\n",
    "                data_dict['value'].append(val)\n",
    "\n",
    "    new_data = pd.DataFrame(data_dict)\n",
    "    print(new_data['timestamp'].diff().dt.days.unique())\n",
    "    return new_data\n",
    "\n",
    "def merge_timeseries_dataframes(dataframes):\n",
    "\n",
    "    return reduce(lambda left, right: pd.merge(left, right, on='timestamp', how='inner'), dataframes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "column_names = [\n",
    "    'ID', 'PARAM', 'TYPE', 'YEAR', 'DD',\n",
    "    'Jan', 'Jan_SYM', 'Feb', 'Feb_SYM', 'Mar', 'Mar_SYM',\n",
    "    'Apr', 'Apr_SYM', 'May', 'May_SYM', 'Jun', 'Jun_SYM',\n",
    "    'Jul', 'Jul_SYM', 'Aug', 'Aug_SYM', 'Sep', 'Sep_SYM',\n",
    "    'Oct', 'Oct_SYM', 'Nov', 'Nov_SYM', 'Dec', 'Dec_SYM'\n",
    "]\n",
    "data = pd.read_csv('data/credit_hydrometric_data_all_stations.csv', skiprows=2, names=column_names)\n",
    "\n",
    "station_data_split = {station: data[data['ID'] == station] for station in data['ID'].unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  nan   31.   28.   30. -333.   59.   61.    1.   29. -334.   60.]\n",
      "[  nan   31.   28.   30. -333.   59.   61.    1.   29. -334.   60.]\n",
      "[  nan   31.   29.   30. -334.   60.   61.    1.   28. -333.   59.]\n",
      "[  nan   31.   28.   30. -333.   59.   61.    1.   29. -334.   60.]\n",
      "[  nan   31.   28.   30. -333.   59.   61.    1.   29. -334.   60.]\n",
      "[  nan   31.   28.   30. -333.   59.   61.    1.   29. -334.   60.]\n",
      "[  nan   31.   28.   30. -333.   59.   61.    1.   29. -334.   60.  366.]\n",
      "[  nan   31.   28.   30. -333.   59.   61.    1.   29. -334.   60.  366.]\n",
      "[  nan   31.   28.   30. -333.   59.   61.    1.   29. -334.   60.]\n",
      "[  nan   31.   28.   30. -333.   59.   61.    1.   29. -334.   60.]\n",
      "[  nan   31.   28.   30. -333.   59.   61.    1.   29. -334.   60.]\n",
      "[  nan   31.   28.   30. -333.   59.   61.    1.   29. -334.   60.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# simple propagating fill in both directions\n",
    "for station_id, station_data in station_data_split.items():\n",
    "    station_data_split[station_id] = station_data.ffill().bfill()\n",
    "    assert station_data_split[station_id].isnull().sum().sum() == 0\n",
    "\n",
    "# Daily Discharge (m3/s) (PARAM = 1) and Daily Water Level (m) (PARAM = 2)\n",
    "timeseries_station_params_split = {}\n",
    "for station_id, station_data in station_data_split.items():\n",
    "    params_split = split_by_param(station_data, 'PARAM')\n",
    "    timeseries_station_params_split[station_id] = {param: convert_to_timeseries(params_split[param], MONTHS) for param in params_split}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# rename columns to include station_id and param_name\n",
    "for station_id, params_timeseries in timeseries_station_params_split.items():\n",
    "    for param_name, params_timeseries in params_timeseries.items():\n",
    "        if param_name == 1:\n",
    "            name = 'discharge'\n",
    "        elif param_name == 2:\n",
    "            name = 'water_level'\n",
    "        params_timeseries.rename(columns={'value': f'{name}_{station_id}'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = [df for params_timeseries in timeseries_station_params_split.values() for df in params_timeseries.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  nan   31.   28.   30. -333.   59.   61.    1.   29. -334.   60.  366.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Merge all timeseries dataframes\n",
    "df_merged = merge_timeseries_dataframes(all_dfs)\n",
    "\n",
    "print(df_merged['timestamp'].diff().dt.days.unique())\n",
    "\n",
    "#df_merged.set_index('timestamp', inplace=True)\n",
    "\n",
    "df_merged.sort_values(by='timestamp', inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous row: 2014-12-31 00:00:00\n",
      "Current row: 2016-01-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "def print_rows_with_gap(df):\n",
    "    prev_timestamp = None\n",
    "    prev_row = None\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        current_timestamp = row['timestamp']\n",
    "\n",
    "        if prev_timestamp is not None:\n",
    "            diff = (current_timestamp - prev_timestamp).days\n",
    "            if diff > 1:\n",
    "                print(f'Previous row: {prev_row.timestamp}')\n",
    "                print(f'Current row: {row.timestamp}')\n",
    "\n",
    "        prev_timestamp = current_timestamp\n",
    "        prev_row = row\n",
    "\n",
    "# Usage:\n",
    "# df is your DataFrame\n",
    "print_rows_with_gap(df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ nan   1. 366.]\n"
     ]
    }
   ],
   "source": [
    "print(df_merged['timestamp'].diff().dt.days.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split df_merged on year >= 2014-12-31 00:00:00 and <2016-01-01 00:00:00\n",
    "df_14 = df_merged[df_merged['timestamp'] < '2014-12-31']\n",
    "df_16= df_merged[df_merged['timestamp'] >= '2016-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan  1.]\n",
      "[nan  1.]\n"
     ]
    }
   ],
   "source": [
    "print(df_14['timestamp'].diff().dt.days.unique())\n",
    "print(df_16['timestamp'].diff().dt.days.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    column_names = [\n",
    "        'ID', 'PARAM', 'TYPE', 'YEAR', 'DD',\n",
    "        'Jan', 'Jan_SYM', 'Feb', 'Feb_SYM', 'Mar', 'Mar_SYM',\n",
    "        'Apr', 'Apr_SYM', 'May', 'May_SYM', 'Jun', 'Jun_SYM',\n",
    "        'Jul', 'Jul_SYM', 'Aug', 'Aug_SYM', 'Sep', 'Sep_SYM',\n",
    "        'Oct', 'Oct_SYM', 'Nov', 'Nov_SYM', 'Dec', 'Dec_SYM'\n",
    "    ]\n",
    "    data = pd.read_csv(filepath, skiprows=2, names=column_names)\n",
    "    return data\n",
    "\n",
    "def preprocess_data(data, station_id):\n",
    "    df = data[data['ID'] == station_id].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def convert_to_timeseries(df, value_column='value'):\n",
    "    # Conversion logic here\n",
    "    pass\n",
    "\n",
    "def merge_datasets(list_of_datasets):\n",
    "    return reduce(lambda left, right: pd.merge(left, right, on=['timestamp'], how='outer'), list_of_datasets)\n",
    "\n",
    "def add_lag_times(df, lag_times):\n",
    "    df_copy = df.copy()\n",
    "    for column, lag in lag_times.items():\n",
    "        df_copy[f'{column}_lagged'] = df[column].shift(lag)\n",
    "    return df_copy\n",
    "\n",
    "def interpolate_hourly(df):\n",
    "    return df.resample('6H').interpolate('linear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lstm(input_shape):\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(100, return_sequences=True, input_shape=input_shape, dropout=0.2, recurrent_dropout=0.2)),\n",
    "        BatchNormalization(),\n",
    "        Bidirectional(LSTM(50, dropout=0.2, recurrent_dropout=0.2)),\n",
    "        BatchNormalization(),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def make_gru(input_shape):\n",
    "    model = Sequential([\n",
    "        GRU(50, return_sequences=True, input_shape=input_shape, dropout=0.2),\n",
    "        LayerNormalization(),\n",
    "        GRU(25, dropout=0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def make_conv_lstm(input_shape):\n",
    "    model = Sequential([\n",
    "        ConvLSTM1D(filters=32, kernel_size=3, return_sequences=True, input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        Flatten(),\n",
    "        Dense(50, activation='relu'),\n",
    "        Dropout(0.25),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_hourly' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m datasets \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmerged\u001b[39m\u001b[38;5;124m'\u001b[39m: df_merged,\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhourly\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mdf_hourly\u001b[49m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlagged\u001b[39m\u001b[38;5;124m'\u001b[39m: df_hourly_lagged\n\u001b[1;32m      5\u001b[0m }\n\u001b[1;32m      7\u001b[0m models \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m: make_lstm,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGRU\u001b[39m\u001b[38;5;124m'\u001b[39m: make_gru,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConvLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m: make_conv_lstm\n\u001b[1;32m     11\u001b[0m }\n\u001b[1;32m     13\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_hourly' is not defined"
     ]
    }
   ],
   "source": [
    "datasets = {\n",
    "    'merged': df_merged,\n",
    "    'hourly': df_hourly,\n",
    "    'lagged': df_hourly_lagged\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'LSTM': make_lstm,\n",
    "    'GRU': make_gru,\n",
    "    'ConvLSTM': make_conv_lstm\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for dataset_name, df in datasets.items():\n",
    "    X_train, X_test, Y_train, Y_test = prepare_dataset(df)  # Define this function based on your dataset structure\n",
    "    for model_name, model_builder in models.items():\n",
    "        model = model_builder((X_train.shape[1],))\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.001))\n",
    "        history = model.fit(X_train, Y_train, validation_split=0.2, epochs=10, batch_size=256, verbose=2)\n",
    "        eval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flood",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
